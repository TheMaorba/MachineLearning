{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fad44c05-03cd-4c1e-bd4a-b5fabe45c053",
   "metadata": {},
   "source": [
    "# **Regresiones lineales y regresiones logísticas.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad015139-9a95-4e9d-a7d0-47a4e2074e3c",
   "metadata": {},
   "source": [
    "# Regresión lineal vs. regresión logística: qué son y cuándo usarlas\n",
    "\n",
    "En aprendizaje supervisado, una “regresión” es un modelo que relaciona una variable objetivo con un conjunto de variables explicativas (features). Dos de las regresiones más usadas son la **regresión lineal** y la **regresión logística**. Aunque comparten el nombre y la idea de modelar una relación entre variables, se aplican a objetivos diferentes y se entrenan con criterios distintos.\n",
    "\n",
    "## Regresión lineal\n",
    "\n",
    "La **regresión lineal** modela una variable objetivo **continua** (numérica) como una combinación lineal de las variables explicativas. En su forma más simple (una sola variable explicativa), busca aproximar la relación:\n",
    "\n",
    "$$\n",
    "y \\approx \\beta_0 + \\beta_1 x\n",
    "$$\n",
    "\n",
    "y en el caso multivariable:\n",
    "\n",
    "$$\n",
    "y \\approx \\beta_0 + \\beta_1 x_1 + \\cdots + \\beta_p x_p\n",
    "$$\n",
    "\n",
    "El entrenamiento se hace típicamente minimizando la suma de cuadrados de los residuos (errores) entre el valor real y el predicho, lo que lleva al enfoque de **mínimos cuadrados**.\n",
    "\n",
    "### ¿Cuándo se usa?\n",
    "Se usa cuando el resultado que queremos predecir es un valor real continuo, por ejemplo:\n",
    "- Precio de una vivienda\n",
    "- Temperatura, humedad o concentración de un contaminante\n",
    "- Demanda de energía o consumo\n",
    "- Tiempo de entrega, duración de un proceso\n",
    "\n",
    "### ¿Qué tipo de casos explica bien?\n",
    "- Relaciones aproximadamente lineales (o que pueden hacerse “casi lineales” con transformaciones).\n",
    "- Situaciones donde interesa interpretar el efecto promedio de una variable sobre la respuesta (interpretabilidad de coeficientes).\n",
    "- Escenarios donde un modelo base y estable es preferible antes de usar modelos más complejos.\n",
    "\n",
    "## Regresión logística\n",
    "\n",
    "La **regresión logística** se usa cuando la variable objetivo es **binaria** (dos clases), típicamente codificada como $y \\in \\{0,1\\}$. En lugar de predecir directamente un valor real, la logística modela una **probabilidad**:\n",
    "\n",
    "$$\n",
    "p(x) = \\Pr(y=1 \\mid x)\n",
    "$$\n",
    "\n",
    "usando la función sigmoide:\n",
    "\n",
    "$$\n",
    "p(x) = \\sigma(\\beta_0 + \\beta_1 x_1 + \\cdots + \\beta_p x_p)\n",
    "\\quad \\text{donde} \\quad\n",
    "\\sigma(z)=\\frac{1}{1+e^{-z}}\n",
    "$$\n",
    "\n",
    "Una forma útil de interpretarla es que modela el **log-odds** (logaritmo de las probabilidades relativas) como lineal en $x$:\n",
    "\n",
    "$$\n",
    "\\log\\left(\\frac{p(x)}{1-p(x)}\\right)=\\beta_0+\\beta_1 x_1+\\cdots+\\beta_p x_p\n",
    "$$\n",
    "\n",
    "El entrenamiento se realiza por **máxima verosimilitud** (equivalente a minimizar la pérdida de entropía cruzada o *cross-entropy*), no por mínimos cuadrados.\n",
    "\n",
    "### ¿Cuándo se usa?\n",
    "Se usa cuando queremos clasificar entre dos clases, por ejemplo:\n",
    "- Aprobado / no aprobado\n",
    "- Enfermo / sano\n",
    "- Fraude / no fraude\n",
    "- Cliente se va (churn) / cliente se queda\n",
    "- Evento ocurre / no ocurre\n",
    "\n",
    "### ¿Qué tipo de casos explica bien?\n",
    "- Problemas de clasificación binaria donde se necesita una probabilidad interpretable.\n",
    "- Situaciones en las que el costo de falsos positivos y falsos negativos es importante (porque se puede ajustar el umbral de decisión).\n",
    "- Casos donde interesa interpretar efectos mediante *odds ratios* ($e^{\\beta_j}$).\n",
    "\n",
    "## Diferencia clave entre ambas\n",
    "\n",
    "- **Regresión lineal**: predice un número real $ \\hat{y} \\in \\mathbb{R} $ (variable continua).\n",
    "- **Regresión logística**: predice una probabilidad $ \\hat{p} \\in (0,1) $ y luego clasifica usando un umbral (por ejemplo 0.5), para objetivos binarios.\n",
    "\n",
    "## Regla práctica rápida para escoger\n",
    "\n",
    "- Si tu objetivo es un **valor continuo** → empieza con **regresión lineal**.\n",
    "- Si tu objetivo son **dos clases** (sí/no, 0/1) → usa **regresión logística**.\n",
    "- En ambos casos, es buena práctica comparar contra un **baseline**, validar con un esquema correcto (train/test o cross-validation) y revisar supuestos y estabilidad antes de concluir.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc8050db-4f74-4589-8b24-f8cf392c3ebf",
   "metadata": {},
   "source": [
    "# **Vamos a ello:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9955da09-19b1-47fc-835f-a2557f494457",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score, accuracy_score, roc_auc_score, confusion_matrix\n",
    "\n",
    "# =========================================================\n",
    "# 1) Regresión lineal: y = beta0 + beta1*x + ruido\n",
    "# =========================================================\n",
    "def make_linear_data(n=120, beta0=2.0, beta1=3.5, x_min=-2.0, x_max=2.0, noise_std=1.0, seed=42):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    x = rng.uniform(x_min, x_max, size=n)\n",
    "    eps = rng.normal(0.0, noise_std, size=n)\n",
    "    y = beta0 + beta1 * x + eps\n",
    "    X = x.reshape(-1, 1)\n",
    "    return X, y\n",
    "\n",
    "# =========================================================\n",
    "# 2) Regresión logística: y ~ Bernoulli(sigmoid(beta0 + b1*x1 + b2*x2))\n",
    "# =========================================================\n",
    "def sigmoid(z):\n",
    "    return 1.0 / (1.0 + np.exp(-z))\n",
    "\n",
    "def make_logistic_data(n=300, beta0=-0.3, beta=(2.0, -1.2), x_std=1.0, seed=123):\n",
    "    \"\"\"\n",
    "    Genera 2 features para poder graficar frontera de decisión.\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "    X = rng.normal(0.0, x_std, size=(n, 2))\n",
    "    z = beta0 + beta[0] * X[:, 0] + beta[1] * X[:, 1]\n",
    "    p = sigmoid(z)\n",
    "    y = rng.binomial(1, p, size=n)\n",
    "    return X, y, p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb4b2131-71b5-4d37-8e03-ac530e04ac34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================\n",
    "# A) Demo regresión lineal\n",
    "# =========================================================\n",
    "X_lin, y_lin = make_linear_data(\n",
    "    n=120,\n",
    "    beta0=2.0,\n",
    "    beta1=3.5,\n",
    "    x_min=-2.0,\n",
    "    x_max=2.0,\n",
    "    noise_std=1.0,\n",
    "    seed=7\n",
    ")\n",
    "#np.ravel(X_lin)\n",
    "pd.DataFrame({\"X\":np.ravel(X_lin),\"Y\":np.ravel(y_lin)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e146a98-894c-4abd-a8ef-db8cbb40d79f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtr, Xte, ytr, yte = train_test_split(X_lin, y_lin, test_size=0.25, random_state=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "395f991e",
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_model = LinearRegression()\n",
    "lin_model.fit(Xtr, ytr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f417a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat_te = lin_model.predict(Xte)\n",
    "yhat_tr = lin_model.predict(Xtr)\n",
    "\n",
    "print(\"=== REGRESIÓN LINEAL ===\")\n",
    "print(f\"Coeficientes estimados: beta0={lin_model.intercept_:.3f}, beta1={lin_model.coef_[0]:.3f}\")\n",
    "print(f\"RMSE test: {np.sqrt(mean_squared_error(yte, yhat_te)):.3f}\")\n",
    "print(f\"R^2  test: {r2_score(yte, yhat_te):.3f}\")\n",
    "\n",
    "# Gráfica: puntos + recta ajustada\n",
    "x_grid = np.linspace(X_lin.min(), X_lin.max(), 200).reshape(-1, 1)\n",
    "y_grid = lin_model.predict(x_grid)\n",
    "\n",
    "plt.figure(figsize=(7,5))\n",
    "plt.scatter(Xtr[:, 0], ytr, label=\"Train\")\n",
    "plt.scatter(Xte[:, 0], yte, label=\"Test\")\n",
    "plt.plot(x_grid[:, 0], y_grid, label=\"Ajuste lineal\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.title(\"Regresión lineal (datos sintéticos)\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Residuales\n",
    "resid = yte - yhat_te\n",
    "plt.figure(figsize=(7,2))\n",
    "plt.scatter(yhat_te, resid)\n",
    "plt.axhline(0.0)\n",
    "plt.xlabel(\"Predicción (test)\")\n",
    "plt.ylabel(\"Residual (y - ŷ)\")\n",
    "plt.title(\"Residuales (test)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0171fb0-c273-461e-9c18-eee2013d04f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================\n",
    "# B) Demo regresión logística (2D)\n",
    "# =========================================================\n",
    "X_log, y_log, p_true = make_logistic_data(\n",
    "    n=350,\n",
    "    beta0=-0.3,\n",
    "    beta=(2.0, -1.2),\n",
    "    x_std=1.0,\n",
    "    seed=9\n",
    ")\n",
    "\n",
    "len(np.ravel(y_log)), len(np.ravel(X_log)), len(np.ravel(p_true))\n",
    "pd.DataFrame({\"X1\":np.ravel(X_log[:,0]),\"X2\":np.ravel(X_log[:,1]),\"Y\":np.ravel(y_log),\"True\":np.ravel(p_true)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "794d4cc0-4d92-47da-9763-faa2b5346d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtr, Xte, ytr, yte = train_test_split(X_log, y_log, test_size=0.25, random_state=9, stratify=y_log)\n",
    "\n",
    "# Pipeline típico: escalar + logística\n",
    "log_model = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"clf\", LogisticRegression(solver=\"lbfgs\", max_iter=2000))\n",
    "])\n",
    "\n",
    "log_model.fit(Xtr, ytr)\n",
    "\n",
    "proba_te = log_model.predict_proba(Xte)[:, 1]\n",
    "pred_te = (proba_te >= 0.5).astype(int)\n",
    "proba_tr = log_model.predict_proba(Xte)[:, 1]\n",
    "pred_tr = (proba_te >= 0.5).astype(int)\n",
    "\n",
    "print(\"\\n=== REGRESIÓN LOGÍSTICA ===\")\n",
    "print(f\"Accuracy test: {accuracy_score(yte, pred_te):.3f}\")\n",
    "print(f\"ROC-AUC  test: {roc_auc_score(yte, proba_te):.3f}\")\n",
    "print(\"Matriz de confusión (test):\")\n",
    "print(confusion_matrix(yte, pred_te))\n",
    "\n",
    "# Frontera de decisión (en el espacio original)\n",
    "# Creamos malla sobre X1-X2 y evaluamos proba\n",
    "x1_min, x1_max = X_log[:, 0].min() - 0.5, X_log[:, 0].max() + 0.5\n",
    "x2_min, x2_max = X_log[:, 1].min() - 0.5, X_log[:, 1].max() + 0.5\n",
    "xx1, xx2 = np.meshgrid(np.linspace(x1_min, x1_max, 250),\n",
    "                       np.linspace(x2_min, x2_max, 250))\n",
    "grid = np.c_[xx1.ravel(), xx2.ravel()]\n",
    "grid_proba = log_model.predict_proba(grid)[:, 1].reshape(xx1.shape)\n",
    "\n",
    "plt.figure(figsize=(7,5))\n",
    "plt.contourf(xx1, xx2, grid_proba, levels=20)\n",
    "plt.colorbar(label=\"P(y=1 | x)\")\n",
    "plt.scatter(Xte[:, 0], Xte[:, 1], c=yte, edgecolor=\"k\")\n",
    "plt.xlabel(\"x1\")\n",
    "plt.ylabel(\"x2\")\n",
    "plt.title(\"Regresión logística: probabilidad y frontera (test)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80faa8f2-bd77-4db2-b398-c0f4dbf78b37",
   "metadata": {},
   "source": [
    "# **Usemos un paquete ya listo....**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b41bb703-2e21-469f-a332-873c0642fc56",
   "metadata": {},
   "source": [
    "## **Regresion Lineal...**\n",
    "Usando un dataset conocido y famoso, ¿Cómo varía el peso del cerebro (cabeza) en función del tamaño que tiene..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29a2a500-703d-49ca-9403-e3427d8c14d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a87b70-c231-4cb9-8f56-6d1ebe9f1b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fitting the model\n",
    "#Para los datos aqui usamos este dataset: https://media.geeksforgeeks.org/wp-content/cdn-uploads/20220522225116/headbrain11.csv\n",
    "df1=pd.read_csv(\"https://media.geeksforgeeks.org/wp-content/cdn-uploads/20220522225116/headbrain11.csv\")\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22a70e2c-3f43-469b-9d35-44d684b416cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y=df1[\"Head Size(cm^3)\"],df1[\"Brain Weight(grams)\"]\n",
    "X = sm.add_constant(X)\n",
    "model = sm.OLS(y,X)\n",
    "results = model.fit()\n",
    "results.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "378be1e9-2970-4988-924a-f8aff1d90a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "results.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42409f94-dc0f-4115-a6c5-d2ca9ee7a32d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid para la recta: debe tener const + x\n",
    "x_grid = np.linspace(df1[\"Head Size(cm^3)\"].min(),\n",
    "                     df1[\"Head Size(cm^3)\"].max(), 200)\n",
    "X_grid = sm.add_constant(pd.DataFrame({\"Head Size(cm^3)\": x_grid}))\n",
    "\n",
    "y_grid = results.predict(X_grid)\n",
    "\n",
    "plt.figure(figsize=(7,5))\n",
    "plt.scatter(df1[\"Head Size(cm^3)\"], y, label=\"Datos\")\n",
    "plt.plot(x_grid, y_grid, label=\"Ajuste lineal\")\n",
    "plt.xlabel(\"Head Size (cm^3)\")\n",
    "plt.ylabel(\"Brain Weight (grams)\")\n",
    "plt.title(\"Regresión lineal: Head Size vs Brain Weight\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "532584f3-969a-4abc-9f8d-1ef7a8c01998",
   "metadata": {},
   "source": [
    "## **Regresión logistica...**\n",
    "Usando un dataset también: Predecimos si un estudiante será admitido en la universidad según su puntuación en el GMAT, su promedio general (GPA) y su experiencia laboral. La variable objetivo es binaria, es decir, admitido o no."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb581753-6898-47c1-b535-585757e812ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Para los datos aqui usamos este dataset: https://media.geeksforgeeks.org/wp-content/uploads/20251025141423714536/logit_train1.csv\n",
    "df = pd.read_csv('https://media.geeksforgeeks.org/wp-content/uploads/20251025141423714536/logit_train1.csv', index_col = 0)\n",
    "\n",
    "#Define Dependent and Independent Variable\n",
    "#Defining dependent and independent variables for training.\n",
    "Xtrain = df[['gmat', 'gpa', 'work_experience']]\n",
    "ytrain = df[['admitted']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c28d8b9-338c-41a1-b0a9-998e1aaac127",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ffd3eae-9180-4f6a-95eb-88a9b60bdb7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "487879d0-458e-4777-b7c7-706c04718f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain = df[['gmat', 'gpa', 'work_experience']][0:15]\n",
    "ytrain = df[['admitted']][0:15]\n",
    "Xtest = df[['gmat', 'gpa', 'work_experience']][15:]\n",
    "ytest = df[['admitted']][15:]\n",
    "\n",
    "\n",
    "log_reg = sm.Logit(ytrain, Xtrain).fit()\n",
    "#Perform Predictions\n",
    "#Performing predictions on testing data.\n",
    "yhat = log_reg.predict(Xtest)\n",
    "prediction = list(map(round, yhat))\n",
    "\n",
    "print('Actual values', list(ytest.values))\n",
    "print('Predictions :', prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a4f49b1-027e-4da5-8c0a-6d96bbb6faf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import (confusion_matrix, accuracy_score)\n",
    "cm = confusion_matrix(ytest, prediction) \n",
    "print (\"Confusion Matrix : \\n\", cm) \n",
    "print('Test accuracy = ', accuracy_score(ytest, prediction)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac37dc8-8392-4831-9350-1c2f8e20e565",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
